{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\"/>\n",
    "\n",
    "# Text Classification\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Bu yazıda, metin sınıflandırmasını Python'da uygulamak için adım adım süreç hakkında bilgi vereceğim. <br>\n",
    "Bu yazıda Python'da denetimli bir öğrenme olan metin sınıflandırma modeli oluşturmaya odaklanıyoruz.<br>\n",
    "Bu yazıda, metin sınıflandırmasının gerçek dünyadan bir örneğini göreceğiz. <br>\n",
    "\n",
    "<img class=\"aligncenter wp-image-43691 size-full\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Untitled-Diagram.png\" alt=\"\" width=\"461\" height=\"200\" srcset=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Untitled-Diagram.png 461w, https://cdn.analyticsvidhya.com/wp-content/uploads/2018/04/Untitled-Diagram-300x130.png 300w\" sizes=\"(max-width: 461px) 100vw, 461px\">\n",
    "\n",
    "<hr> \n",
    "\n",
    "* [1. Text Classification Nedir ?](#1)\n",
    "* [2. Dataset Hakkında](#2)\n",
    "* [3. Sentiment Analysis with Scikit-Learn](#3)\n",
    "* [4. Importing Libraries](#4)\n",
    "* [5. Importing The dataset](#5)\n",
    "* [6. Text Preprocessing](#6)\n",
    "* [7. Converting Text to Numbers](#7)\n",
    "* [8. Training and Test Sets](#8)\n",
    "* [9. Training Text Classification Model and Predicting Sentiment](#9)\n",
    "* [10. Evaluating The Model](#10)\n",
    "* [11. Saving and Loading the Model](#11)\n",
    "\n",
    "\n",
    "\n",
    "* Python'da Metin Sınıflandırmasını Anlamak ve Uygulamak İçin Kapsamlı Bir Kılavuz Olusturalım\n",
    "    * 1. Dataset Preparation: \n",
    "        İlk adım, bir veri kümesinin yüklenmesi ve temel ön işlemlerin gerçekleştirilmesi işlemini içeren Veri Kümesi Hazırlama aşamasıdır.\n",
    "        \n",
    "    * 2. Feature Engineering: \n",
    "        Bir sonraki adım, ham veri kümesinin bir makine öğrenme modelinde kullanılabilecek düz özelliklere dönüştürüldüğü Özellik Mühendisliği'dir. Bu adım aynı zamanda mevcut verilerden yeni özellikler oluşturma işlemini de içerir.\n",
    "    \n",
    "    * 3. Model Training: \n",
    "        Son adım, bir makine öğrenme modelinin etiketli bir veri kümesi üzerinde eğitildiği Model Oluşturma adımıdır.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Text Classification Nedir ?\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Metin sınıflandırma Doğal Dil İşlemede en önemli görevlerden biridir. Metin dizelerini veya belgeleri, dizelerin içeriğine bağlı olarak farklı kategorilere ayırma işlemidir.<br>\n",
    "Metin sınıflandırması, bir e-postayı spam veya spam değil olarak sınıflandırma, blog yayınlarını farklı kategorilere ayırma, müşteri sorgularının otomatik etiketlenmesi gibi çeşitli uygulamalara sahiptir.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Dataset Hakkında\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "Bu makale için kullanacağımız veri seti Cornell Doğal Dil İşleme Grubundan indirilebilir. <br>\n",
    "Veri kümesi toplam 2000 belgeden oluşmaktadır.<br>\n",
    "Belgelerin yarısı bir filmle ilgili olumlu eleştiriler, geri kalan yarısı ise negatif incelemeler içeriyor.<br>\n",
    "Veri kümesini indirdikten sonra açın veya çıkarın. \"txt_sentoken\" klasörünü açın. <br>\n",
    "Klasör iki alt klasör içerir: \"neg\" ve \"pos\". Bu klasörleri açarsanız, film incelemeleri içeren metin belgelerini görebilirsiniz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Sentiment Analysis with Scikit-Learn\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Verileri indirdiğimize göre, şimdi bazı eylemleri görme zamanı. Bu bölümde, farklı filmlerin incelemelerinden gelen tahminleri tahmin etmek için bir dizi adım gerçekleştireceğiz. Bu adımlar herhangi bir metin sınıflandırma görevi için kullanılabilir. <br>\n",
    "\n",
    "Bir metin sınıflandırma modelini eğitmek için makine öğrenimi için Python'un Scikit-Learn kütüphanesini kullanacağız. <br>\n",
    "Python'da bir metin sınıflandırma modeli oluşturmak için gerekli adımlar şunlardır:<br>\n",
    "\n",
    "* Importing Libraries\n",
    "* Importing The dataset\n",
    "* Text Preprocessing\n",
    "* Converting Text to Numbers\n",
    "* Training and Test Sets\n",
    "* Training Text Classification Model and Predicting Sentiment\n",
    "* Evaluating The Model\n",
    "* Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Importing Libraries\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerekli kütüphanelerin indirilmesi ile baslayalım !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yazilimrehberi2020/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Importing The dataset\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Veri kümesini uygulamamıza içe aktarmak için sklearn_datasets kütüphanesindeki load_files işlevini kullanacağız.<br>\n",
    "Load_files işlevi, veri kümesini otomatik olarak data ve target kümelerine böler.<br>\n",
    "\"txt_sentoken\" dizinine gelelim.<br>\n",
    "<b>load_files<b>, \"txt_sentoken\" klasörü içindeki her klasörü tek bir kategori olarak ele alır ve bu klasördeki tüm belgelere karşılık gelen kategorisi atanır.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(\"txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Yukarıdaki kodda, load_files fonksiyonu hem \"neg\" hem de \"pos\" klasörlerindeki verileri X değişkenine yüklerken target kategoriler y'de saklanır.<br>\n",
    "Burada X, her öğenin tek kullanıcı incelemesine karşılık geldiği 2000 dize türü öğelerin bir listesidir. Benzer şekilde, y 2000 büyüklüğünde sayısal bir dizidir. Ekranda y yazdırırsanız, 1s ve 0s bir dizi görürsünüz.<br>\n",
    "Bunun nedeni, her kategori için load_files işlevinin hedef numpy dizisine bir sayı eklemesidir. İki kategorimiz var: \"neg\" ve \"pos\", bu nedenle hedef diziye 1'ler ve 0'lar eklendi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Text Preprocessing\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "Veri kümesi içe aktarıldıktan sonra, sonraki adım metnin önişlemini yapmaktır. <br>\n",
    "Metin sayılar, özel karakterler ve istenmeyen boşluklar içerebilir. <br>\n",
    "Karşılaştığımız soruna bağlı olarak, bu özel karakterleri ve sayıları metinden kaldırmamız gerekebilir veya olmayabilir.<br>\n",
    "Ancak, açıklama uğruna metnimizden tüm özel karakterleri, sayıları ve istenmeyen boşlukları kaldıracağız. Verileri önceden işlemek için aşağıdaki komut dosyasını yürütün:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yazilimrehberi2020/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\" size=\"2\"/>\n",
    "* Yukarıdaki komut dosyasında, farklı önişleme görevleri gerçekleştirmek için Python re kütüphanesinden Regex İfadeleri kullanıyoruz.<br>\n",
    "* Özel karakterler, sayılar vb. sözcük olmayan tüm karakterleri kaldırarak başlıyoruz.<br>\n",
    "* Sonra, tüm tek karakterleri kaldırıyoruz. Örneğin, noktalama işaretini \"David'in\" den kaldırdığımızda ve bir boşlukla değiştirdiğimizde, \"David\" ve hiçbir anlamı olmayan tek bir karakter \"s\" elde ederiz.<br>\n",
    "* Bu tür tek karakterleri kaldırmak için, her iki tarafında boşluk olan tüm tek karakterleri tek bir boşlukla değiştiren \\ s + [a-zA-Z] \\ s + normal ifadesini kullanırız.<br>\n",
    "* Ardından, <b> belgenin başlangıcından tek bir karakteri tek bir boşlukla değiştirmek için \\ ^ [a-zA-Z] \\ s + normal ifadesini kullanırız.</b> Tek karakterlerin tek bir boşlukla değiştirilmesi, ideal olmayan birden çok boşluk ile sonuçlanabilir. <br>\n",
    "* Bir veya daha fazla boşluğu tek bir boşlukla değiştirmek için \\ s + normal ifadesini tekrar kullanırız. Bayt biçiminde bir veri kümeniz olduğunda, her dizeden önce \"b\" alfabe harfi eklenir. Normal ifade ^ b \\ s +, \"b\" yi bir dizenin başından kaldırır. <br>\n",
    "* Bir sonraki adım, verileri gerçekte aynı olan ancak farklı vakaları olan kelimelerin eşit olarak ele alınabilmesi için küçük harfe dönüştürmektir. <br>\n",
    "<hr>\n",
    "* Son ön işleme aşaması, lemmatizasyondur. Lemmatizasyonda kelimeyi sözlük kök formuna indiririz. Örneğin, \"kediler\", \"kedi\" ye dönüştürülür. Lemmatizasyon, anlamsal olarak benzer ancak sözdizimsel olarak farklı özellikler oluşturmaktan kaçınmak için yapılır. Örneğin, semantik olarak benzer olan \"kediler\" ve \"kedi\" adında iki farklı özellik istemiyoruz, bu nedenle lemmatizasyon gerçekleştiriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Converting Text to Numbers\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Makineler, insanların aksine, ham metni anlayamazlar. Makineler yalnızca sayıları görebilir. Özellikle, makine öğrenimi gibi istatistiksel teknikler sadece rakamlarla başa çıkabilir. Bu nedenle, metnimizi sayılara dönüştürmemiz gerekiyor. <br><br>\n",
    "\n",
    "Metni karşılık gelen sayısal formata dönüştürmek için farklı yaklaşımlar mevcuttur. Kelime Torbası Modeli (BoW) ve Kelime Gömme (Word Embeddings) Modeli en yaygın kullanılan yaklaşımlardan ikisidir. Bu yazıda, metnimizi sayılara dönüştürmek için kelime torbası modelini kullanacağız.\n",
    "<hr>\n",
    "<b> Bag of Words </b> <br>\n",
    "Aşağıdaki komut dosyası, metin belgelerini karşılık gelen sayısal özelliklere dönüştürmek için kelime torbası modelini kullanır:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\" size=\"2\"/>\n",
    "\n",
    "İlk parametre 1500'e ayarlanmış olan max_features parametresidir. <br>\n",
    "Bunun nedeni, kelimeleri kelime torbası yaklaşımını kullanarak sayılara dönüştürdüğünüzde, tüm belgelerdeki tüm benzersiz kelimelerin özelliklere dönüştürülmesidir. <br>\n",
    "Tüm belgeler on binlerce benzersiz kelime içerebilir. <br>\n",
    "Ancak, görülme sıklığı çok düşük olan kelimeler alışılmadık şekilde belgeleri sınıflandırmak için iyi bir parametre değildir. <br>\n",
    "Bu nedenle max_features parametresini 1500 olarak ayarlıyoruz, yani sınıflandırıcımızı eğitmek için en çok 1500 kelimeyi kullanmak istiyoruz. <br>\n",
    "Sonraki parametre min_df şeklindedir ve 5 olarak ayarlanmıştır. <br>\n",
    "Bu, bu özelliği içermesi gereken minimum belge sayısına karşılık gelir. <br>\n",
    "Bu yüzden sadece en az 5 belgede yer alan kelimeleri dahil ediyoruz. <br>\n",
    "Benzer şekilde, max_df özelliği için değer 0,7 olarak ayarlanır; burada fraksiyon bir yüzdeye karşılık gelir. <br>\n",
    "Burada 0.7, yalnızca tüm belgelerin maksimum% 70'inde meydana gelen kelimeleri eklememiz gerektiği anlamına gelir. <br>\n",
    "Hemen hemen her belgede ortaya çıkan kelimeler genellikle sınıflandırma için uygun değildir, çünkü belge hakkında benzersiz bilgi vermezler. <br>\n",
    "Son olarak, duyarlılık analizi söz konusu olduğunda, durdurma kelimeleri yararlı bilgiler içermeyebileceğinden, durdurma kelimelerini metnimizden kaldırırız. <br>\n",
    "Durdurma sözcüklerini kaldırmak için, durma sözcükleri nesnesini nltk.corpus kitaplığından stop_wordsparameter öğesine geçiririz. <br>\n",
    "CountVectorizer sınıfının fit_transform işlevi metin belgelerini karşılık gelen sayısal özelliklere dönüştürür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "## TF/IDF\n",
    "\n",
    "* Kelime torbası yaklaşımı, metni sayılara dönüştürmek için iyi çalışır. Ancak, bir dezavantajı vardır. Belirli bir belgedeki oluşumuna göre bir kelimeye bir puan atar. <br>\n",
    "* Sözcüğün diğer belgelerde de yüksek bir sıklığa sahip olabileceği dikkate alınmaz. TFIDF, bir kelimenin terim sıklığını ters belge sıklığı ile çarparak bu sorunu giderir.\n",
    "\n",
    "Sıklık terimi şu şekilde hesaplanır :<br>\n",
    "***Term frequency = (Number of Occurrences of a word)/(Total words in the document)*** <br>\n",
    "Ters Belge Sıklığı şu şekilde hesaplanır :<br>\n",
    "***IDF(word) = Log((Total number of documents)/(Number of documents containing the word))*** <br>\n",
    "\n",
    "* Belirli bir belgedeki bir sözcüğün TFIDF değeri, söz konusu belgede o kelimenin oluşma sıklığı daha yüksek, ancak diğer tüm belgelerde daha düşükse daha yüksektir.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sözcük torbası modeli kullanılarak elde edilen değerleri TFIDF değerlerine dönüştürmek için aşağıdaki komut dosyasını yürütün\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidfconverter = TfidfTransformer()\n",
    "\n",
    "X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "# Aşağıdaki komut dosyasını kullanarak metin belgelerini doğrudan TFIDF özellik değerlerine de \n",
    "# (önce belgeleri kelime özelliklerine dönüştürmeden) dönüştürebilirsiniz:\n",
    "\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "\n",
    "# X = tfidfconverter.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Training and Test Sets\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Diğer denetimli makine öğrenimi problemleri gibi, verilerimizi eğitim ve test setlerine ayırmamız gerekir. Bunu yapmak için sklearn.model_selection kütüphanesinden train_test_split yardımcı programını kullanacağız. Aşağıdaki komut dosyasını yürütün:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The above script divides data into 20% test set and 80% training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Training Text Classification Model and Predicting Sentiment\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>\n",
    "\n",
    "Verilerimizi eğitim ve test setine ayırdık. Şimdi gerçek eylemi görme zamanı. Modelimizi eğitmek için Rastgele Orman Algoritmasını kullanacağız. İstediğiniz başka bir modeli kullanabilirsiniz. <br>\n",
    "Makine öğrenme modelimizi rastgele orman algoritmasını kullanarak eğitmek için sklearn.ensemble kütüphanesinden RandomForestClassifier sınıfını kullanacağız. Bu sınıfın fit yöntemi algoritmayı eğitmek için kullanılır. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                       n_jobs=None, oob_score=False, random_state=0, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Son olarak, test setimizdeki belgeler için duyarlılığı tahmin etmek için, aşağıda gösterildiği gibi \n",
    "#RandomForestClassifier sınıfının tahmin yöntemini kullanabiliriz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tebrikler, ilk metin sınıflandırma modelinizi başarıyla eğitdiniz ve bazı tahminlerde bulundunuz.\n",
    "# Şimdi yeni oluşturduğunuz modelin performansını görme zamanı."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Evaluating The Model\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  28]\n",
      " [ 30 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       208\n",
      "           1       0.85      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Çıktıdan, modelimizin% 85.5'lik bir doğruluk elde ettiği görülebilir, \n",
    "#bu da CountVectorizer ve rastgele orman algoritmamız için tüm parametreleri rastgele seçtiğimiz için çok iyidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a>\n",
    "\n",
    "<font color=\"blue\"/>\n",
    "\n",
    "# Saving and Loading the Model\n",
    "\n",
    "<font color=\"black\" size=\"3\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'wb') as picklefile:\n",
    "    pickle.dump(classifier,picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_classifier', 'rb') as training_model:\n",
    "    model = pickle.load(training_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[180  28]\n",
      " [ 30 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       208\n",
      "           1       0.85      0.84      0.85       192\n",
      "\n",
      "    accuracy                           0.85       400\n",
      "   macro avg       0.85      0.85      0.85       400\n",
      "weighted avg       0.85      0.85      0.85       400\n",
      "\n",
      "0.855\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = model.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred2))\n",
    "print(classification_report(y_test, y_pred2))\n",
    "print(accuracy_score(y_test, y_pred2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metin sınıflandırması en sık kullanılan NLP görevlerinden biridir. Bu yazıda Python'da metin sınıflandırmasının nasıl yapılabileceğine dair basit bir örnek gördük. Film incelemelerinin duygusal analizini yaptık."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performansı iyileştirip iyileştiremeyeceğinizi görmek için başka bir makine öğrenimi algoritmasını değiştirmenizi öneririm. Ayrıca, herhangi bir gelişme elde edip edemeyeceğinizi görmek için CountVectorizerclass parametrelerini değiştirmeyi deneyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
